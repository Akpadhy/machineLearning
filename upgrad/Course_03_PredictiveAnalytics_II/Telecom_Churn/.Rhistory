#-->We also see a marked drop in the high VIF values such as tnure, monthlycharges etc.
# Null deviance: 5673.9  on 4906  degrees of freedom
# Residual deviance: 4081.3  on 4893  degrees of freedom
# AIC: 4109.3
#--
#--Model 5
#--Removing OnlineBackupYes as the pvalue is high 0.013810
model_5 <-glm(formula = Churn ~ tenure + MonthlyCharges + TotalCharges +
ContractOne.year + ContractTwo.year +
PaperlessBillingYes + PaymentMethodElectronic.check + MultipleLinesYes +
InternetServiceFiber.optic + InternetServiceNo +
StreamingTVYes + StreamingMoviesYes,
family = "binomial", data = data_train_reg)
summary(model_5)
vif(model_5)
#-->We do not see any drop in VIF values also all the remaining variables
#   appear as significant. But many of the values have high VIF and seem to be slightly co-related
#   for example Monthly and Total charges are clearly related from logic and the same goes for tenure
#   so we take the higher p-value and remove it.
# Null deviance: 5673.9  on 4906  degrees of freedom
# Residual deviance: 4087.4  on 4894  degrees of freedom
# AIC: 4113.4
#--
#--Model 6
#--Removing TotalCjharges as it has higher pvalue is 0.000272
model_6 <-glm(formula = Churn ~ tenure + MonthlyCharges +
ContractOne.year + ContractTwo.year +
PaperlessBillingYes + PaymentMethodElectronic.check + MultipleLinesYes +
InternetServiceFiber.optic + InternetServiceNo +
StreamingTVYes + StreamingMoviesYes,
family = "binomial", data = data_train_reg)
summary(model_6)
vif(model_6)
#-->As per our expectations this has reduced the tenure but from the decrase it is
#   clearly more co-related than Monthly charges
# Null deviance: 5673.9  on 4906  degrees of freedom
# Residual deviance: 4101.2  on 4895  degrees of freedom
# AIC: 4125.2
#--
#--Model 7
#--Retaining MonthlyCahrges as it is an important factor from a business point
model_7 <-glm(formula = Churn ~ tenure + MonthlyCharges +
ContractOne.year + ContractTwo.year +
PaperlessBillingYes + PaymentMethodElectronic.check + MultipleLinesYes +
InternetServiceNo +
StreamingTVYes + StreamingMoviesYes,
family = "binomial", data = data_train_reg)
summary(model_7)
vif(model_7)
#-->This has reduced the Monthly Charges also which is understandable as such features
#   generally incur more cost. This though has related is some insignificant variables.
# Null deviance: 5673.9  on 4906  degrees of freedom
# Residual deviance: 4183.5  on 4896  degrees of freedom
# AIC: 4205.5
#--
#--Model 8
#--Removing StreamingTVYes with high PValue 0.11223
model_8 <-glm(formula = Churn ~ tenure + MonthlyCharges +
ContractOne.year + ContractTwo.year +
PaperlessBillingYes + PaymentMethodElectronic.check + MultipleLinesYes +
InternetServiceNo +
StreamingMoviesYes,
family = "binomial", data = data_train_reg)
summary(model_8)
vif(model_8)
# Null deviance: 5673.9  on 4906  degrees of freedom
# Residual deviance: 4186.1  on 4897  degrees of freedom
# AIC: 4206.1
#--
#--Model 9
#--Removing StreamingMoviesYes with high PValue 0.01472
model_9 <-glm(formula = Churn ~ tenure + MonthlyCharges +
ContractOne.year + ContractTwo.year +
PaperlessBillingYes + PaymentMethodElectronic.check + MultipleLinesYes +
InternetServiceNo,
family = "binomial", data = data_train_reg)
summary(model_9)
vif(model_9)
# Null deviance: 5673.9  on 4906  degrees of freedom
# Residual deviance: 4192.0  on 4898  degrees of freedom
# AIC: 4210
#--
#--Model 10
#--Removing InternetServiceNo with high PValue 0.0142
model_10 <-glm(formula = Churn ~ tenure + MonthlyCharges +
ContractOne.year + ContractTwo.year +
PaperlessBillingYes + PaymentMethodElectronic.check + MultipleLinesYes,
family = "binomial", data = data_train_reg)
summary(model_10)
vif(model_10)
# Null deviance: 5673.9  on 4906  degrees of freedom
# Residual deviance: 4198.1  on 4899  degrees of freedom
# AIC: 4214.1
#--
#--Model 11
#--Removing MultipleLinesYes with high PValue 0.0329
model_11 <-glm(formula = Churn ~ tenure + MonthlyCharges +
ContractOne.year + ContractTwo.year +
PaperlessBillingYes + PaymentMethodElectronic.check,
family = "binomial", data = data_train_reg)
summary(model_11)
vif(model_11)
#-->With this model we have all VIF values below threshold of 2 and
#   all insignificant variable removed
# Null deviance: 5673.9  on 4906  degrees of freedom
# Residual deviance: 4202.7  on 4900  degrees of freedom
# AIC: 4216.7
#--Taking Model 11 as the finalised model
final_model <- model_11
summary(final_model)
str(final_model)
#--removing SniorCitzenYes as pvalue is 0.013055
model_2 <-glm(formula = Churn ~ tenure + MonthlyCharges + TotalCharges +
PhoneServiceYes + ContractOne.year + ContractTwo.year +
PaperlessBillingYes + PaymentMethodElectronic.check + MultipleLinesYes +
InternetServiceFiber.optic + InternetServiceNo + OnlineBackupYes +
DeviceProtectionYes + StreamingTVYes + StreamingMoviesYes,
family = "binomial", data = data_train_reg)
summary(model_2)
vif(model_2)
# Null deviance: 5673.9  on 4906  degrees of freedom
# Residual deviance: 4067.1  on 4891  degrees of freedom
# AIC: 4099.1
#--
#--Model 3
#--Removing OnlineBackupYes as the pvalue is high 0.006619
model_3 <-glm(formula = Churn ~ tenure + MonthlyCharges + TotalCharges +
PhoneServiceYes + ContractOne.year + ContractTwo.year +
PaperlessBillingYes + PaymentMethodElectronic.check + MultipleLinesYes +
InternetServiceFiber.optic + InternetServiceNo +
DeviceProtectionYes + StreamingTVYes + StreamingMoviesYes,
family = "binomial", data = data_train_reg)
summary(model_3)
vif(model_3)
# Null deviance: 5673.9  on 4906  degrees of freedom
# Residual deviance: 4074.5  on 4892  degrees of freedom
# AIC: 4104.5
#--
#--Model 4
#--Removing OnlineBackupYes as the pvalue is high 0.006619
model_4 <-glm(formula = Churn ~ tenure + MonthlyCharges + TotalCharges +
ContractOne.year + ContractTwo.year +
PaperlessBillingYes + PaymentMethodElectronic.check + MultipleLinesYes +
InternetServiceFiber.optic + InternetServiceNo +
DeviceProtectionYes + StreamingTVYes + StreamingMoviesYes,
family = "binomial", data = data_train_reg)
summary(model_4)
vif(model_4)
#-->We also see a marked drop in the high VIF values such as tnure, monthlycharges etc.
# Null deviance: 5673.9  on 4906  degrees of freedom
# Residual deviance: 4081.3  on 4893  degrees of freedom
# AIC: 4109.3
#--
#--Model 5
#--Removing OnlineBackupYes as the pvalue is high 0.013810
model_5 <-glm(formula = Churn ~ tenure + MonthlyCharges + TotalCharges +
ContractOne.year + ContractTwo.year +
PaperlessBillingYes + PaymentMethodElectronic.check + MultipleLinesYes +
InternetServiceFiber.optic + InternetServiceNo +
StreamingTVYes + StreamingMoviesYes,
family = "binomial", data = data_train_reg)
summary(model_5)
vif(model_5)
#-->We do not see any drop in VIF values also all the remaining variables
#   appear as significant. But many of the values have high VIF and seem to be slightly co-related
#   for example Monthly and Total charges are clearly related from logic and the same goes for tenure
#   so we take the higher p-value and remove it.
# Null deviance: 5673.9  on 4906  degrees of freedom
# Residual deviance: 4087.4  on 4894  degrees of freedom
# AIC: 4113.4
#--
#--Model 6
#--Removing TotalCjharges as it has higher pvalue is 0.000272
model_6 <-glm(formula = Churn ~ tenure + MonthlyCharges +
ContractOne.year + ContractTwo.year +
PaperlessBillingYes + PaymentMethodElectronic.check + MultipleLinesYes +
InternetServiceFiber.optic + InternetServiceNo +
StreamingTVYes + StreamingMoviesYes,
family = "binomial", data = data_train_reg)
summary(model_6)
vif(model_6)
#-->As per our expectations this has reduced the tenure but from the decrase it is
#   clearly more co-related than Monthly charges
# Null deviance: 5673.9  on 4906  degrees of freedom
# Residual deviance: 4101.2  on 4895  degrees of freedom
# AIC: 4125.2
#--
#--Model 7
#--Retaining MonthlyCahrges as it is an important factor from a business point
model_7 <-glm(formula = Churn ~ tenure + MonthlyCharges +
ContractOne.year + ContractTwo.year +
PaperlessBillingYes + PaymentMethodElectronic.check + MultipleLinesYes +
InternetServiceNo +
StreamingTVYes + StreamingMoviesYes,
family = "binomial", data = data_train_reg)
summary(model_7)
vif(model_7)
#-->This has reduced the Monthly Charges also which is understandable as such features
#   generally incur more cost. This though has related is some insignificant variables.
# Null deviance: 5673.9  on 4906  degrees of freedom
# Residual deviance: 4183.5  on 4896  degrees of freedom
# AIC: 4205.5
#--
#--Model 8
#--Removing StreamingTVYes with high PValue 0.11223
model_8 <-glm(formula = Churn ~ tenure + MonthlyCharges +
ContractOne.year + ContractTwo.year +
PaperlessBillingYes + PaymentMethodElectronic.check + MultipleLinesYes +
InternetServiceNo +
StreamingMoviesYes,
family = "binomial", data = data_train_reg)
summary(model_8)
vif(model_8)
# Null deviance: 5673.9  on 4906  degrees of freedom
# Residual deviance: 4186.1  on 4897  degrees of freedom
# AIC: 4206.1
#--
#--Model 9
#--Removing StreamingMoviesYes with high PValue 0.01472
model_9 <-glm(formula = Churn ~ tenure + MonthlyCharges +
ContractOne.year + ContractTwo.year +
PaperlessBillingYes + PaymentMethodElectronic.check + MultipleLinesYes +
InternetServiceNo,
family = "binomial", data = data_train_reg)
summary(model_9)
vif(model_9)
# Null deviance: 5673.9  on 4906  degrees of freedom
# Residual deviance: 4192.0  on 4898  degrees of freedom
# AIC: 4210
#--
#--Model 10
#--Removing InternetServiceNo with high PValue 0.0142
model_10 <-glm(formula = Churn ~ tenure + MonthlyCharges +
ContractOne.year + ContractTwo.year +
PaperlessBillingYes + PaymentMethodElectronic.check + MultipleLinesYes,
family = "binomial", data = data_train_reg)
summary(model_10)
vif(model_10)
# Null deviance: 5673.9  on 4906  degrees of freedom
# Residual deviance: 4198.1  on 4899  degrees of freedom
# AIC: 4214.1
#--
#--Model 11
#--Removing MultipleLinesYes with high PValue 0.0329
model_11 <-glm(formula = Churn ~ tenure + MonthlyCharges +
ContractOne.year + ContractTwo.year +
PaperlessBillingYes + PaymentMethodElectronic.check,
family = "binomial", data = data_train_reg)
summary(model_11)
vif(model_11)
exp(model_11$coefficients)
exp(exp(coef(model_11)))
exp(exp(coef(final_model)))
svmmodel.probs<-attr(svmpred,"decision.values")
svmmodel.class<-predict(churn.svm.model, data_test$Churn, type="class")
svmmodel.labels<-data_test$Churn
#analyzing result
#roc analysis for test data
svmmodel.prediction<-prediction(svmmodel.probs,svmmodel.labels)
svmmodel.performance<-performance(svmmodel.prediction,"tpr","fpr")
svmmodel.auc<-performance(svmmodel.prediction,"auc")@y.values[[1]]
tune.svm <- tune(svm, Churn~., data = data_train, kernel = "linear",
ranges = list(cost = c(0.001, 0.01, 0.1, 0.5, 1, 10, 100)))
tune.svm <- tune(svm, Churn~., data = data_train, kernel = "linear",
ranges = list(cost = c(0.001, 0.01, 0.1, 0.5, 1, 10, 100)))
timestamp()
##------ Tue Oct 18 23:46:21 2016 ------##
#
summary(tune.svm)
#..............................................................................
# -- best parameters: cost 0.1
# - best performance: 0.1999152
#
# - Detailed performance results:
#   cost     error dispersion
# 1 1e-03 0.2649237 0.02589409
# 2 1e-02 0.2001189 0.01975207
# 3 1e-01 0.1999152 0.01963013
# 4 5e-01 0.2001189 0.01940211
# 5 1e+00 0.2001185 0.01915922
# 6 1e+01 0.2009331 0.01954427
# 7 1e+02 0.2011368 0.01886735
#..............................................................................
#--> The best performance is for model with cost 0.1
churn.svm.model <- tune.svm$best.model
summary(churn.svm.model)
#..............................................................................
# Parameters:
#   SVM-Type:  C-classification
# SVM-Kernel:  linear
#       cost:  0.1
#      gamma:  0.02173913
#
# Number of Support Vectors:  2271 ( 1139 1132 )
# Number of Classes:  2
# Levels: No Yes
#..............................................................................
svmpred <- predict(churn.svm.model, data_test)
table(predicted = svmpred, truth = data_test$Churn)
#..............................................................................
#             truth
# predicted   No   Yes
#        No  1383  260
#       Yes  163   297
#..............................................................................
confusionMatrix(svmpred, data_test$Churn, positive = "Yes")
#..............................................................................
# Confusion Matrix and Statistics
# -------------------------------
#             Reference
# predicted   No   Yes
#        No  1383  260
#       Yes  163   297
#
#            Accuracy : 0.7989
#              95% CI : (0.7772, 0.8121)
#
#          Sensitivity : 0.8946
#          Specificity : 0.5332
#
# 'Positive' Class : No
#..............................................................................
svmmodel.probs<-attr(svmpred,"decision.values")
svmmodel.class<-predict(churn.svm.model, data_test$Churn, type="class")
svmmodel.labels<-data_test$Churn
#analyzing result
#roc analysis for test data
svmmodel.prediction<-prediction(svmmodel.probs,svmmodel.labels)
svmmodel.performance<-performance(svmmodel.prediction,"tpr","fpr")
svmmodel.auc<-performance(svmmodel.prediction,"auc")@y.values[[1]]
churn.svm.model
svmmodel.probs<-attr(svmpred,"decision.values")
svmmodel.class<-predict(churn.svm.model, subset(data_test,select=-Churn), type="class")
svmmodel.labels<-data_test$Churn
svmmodel.prediction<-prediction(svmmodel.probs,svmmodel.labels)
svmmodel.performance<-performance(svmmodel.prediction,"tpr","fpr")
svmmodel.auc<-performance(svmmodel.prediction,"auc")@y.values[[1]]
svmmodel.probs
svmpred
str(svmpred)
svmmodel.probs<-attr(svmpred,"svmpred")
svmmodel.class<-predict(churn.svm.model, subset(data_test,select=-Churn), type="class")
svmmodel.labels<-data_test$Churn
#analyzing result
#roc analysis for test data
svmmodel.prediction<-prediction(svmmodel.probs,svmmodel.labels)
svmpred
svmmodel.probs<-attr(svmpred,"names")
svmmodel.probs
svmmodel.prediction<-prediction(svmmodel.probs,svmmodel.labels)
svmmodel.performance<-performance(svmmodel.prediction,"tpr","fpr")
svmmodel.probs<-attr(svmpred,"names")
svmmodel.class<-predict(churn.svm.model, subset(data_test,select=-Churn), probability = TRUE)
svmmodel.labels<-data_test$Churn
#analyzing result
#roc analysis for test data
svmmodel.prediction<-prediction(svmmodel.probs,svmmodel.labels)
svmmodel.performance<-performance(svmmodel.prediction,"tpr","fpr")
svmmodel.auc<-performance(svmmodel.prediction,"auc")@y.values[[1]]
model <- svm ( Churn~., data=data_train, probability=TRUE, cost = 0.1)
svmpred <- predict(churn.svm.model, data_test)
table(predicted = svmpred, truth = data_test$Churn)
#..............................................................................
#             truth
# predicted   No   Yes
#        No  1383  260
#       Yes  163   297
#..............................................................................
confusionMatrix(svmpred, data_test$Churn, positive = "Yes")
#..............................................................................
# Confusion Matrix and Statistics
# -------------------------------
#             Reference
# predicted   No   Yes
#        No  1383  260
#       Yes  163   297
#
#            Accuracy : 0.7989
#              95% CI : (0.7772, 0.8121)
#
#          Sensitivity : 0.8946
#          Specificity : 0.5332
#
# 'Positive' Class : No
#..............................................................................
svmmodel.probs<-attr(svmpred,"names")
svmmodel.class<-predict(churn.svm.model, subset(data_test,select=-Churn), probability = TRUE)
svmmodel.labels<-data_test$Churn
#analyzing result
#roc analysis for test data
svmmodel.prediction<-prediction(svmmodel.probs,svmmodel.labels)
svmmodel.performance<-performance(svmmodel.prediction,"tpr","fpr")
svmmodel.auc<-performance(svmmodel.prediction,"auc")@y.values[[1]]
svmpred
str(svmpred)
svmpred <- predict(churn.svm.model, data_test, probability=TRUE)
table(predicted = svmpred, truth = data_test$Churn)
#..............................................................................
#             truth
# predicted   No   Yes
#        No  1383  260
#       Yes  163   297
#..............................................................................
confusionMatrix(svmpred, data_test$Churn, positive = "Yes")
#..............................................................................
# Confusion Matrix and Statistics
# -------------------------------
#             Reference
# predicted   No   Yes
#        No  1383  260
#       Yes  163   297
#
#            Accuracy : 0.7989
#              95% CI : (0.7772, 0.8121)
#
#          Sensitivity : 0.8946
#          Specificity : 0.5332
#
# 'Positive' Class : No
#..............................................................................
svmmodel.probs<-attr(svmpred,"names")
svmmodel.class<-predict(churn.svm.model, subset(data_test,select=-Churn), probability = TRUE)
svmmodel.labels<-data_test$Churn
#analyzing result
#roc analysis for test data
svmmodel.prediction<-prediction(svmmodel.probs,svmmodel.labels)
svmmodel.performance<-performance(svmmodel.prediction,"tpr","fpr")
svmmodel.auc<-performance(svmmodel.prediction,"auc")@y.values[[1]]
churn.svm.model <- svm ( Churn~., data=data_train, probability=TRUE, cost = 0.1)
svmpred <- predict(churn.svm.model, data_test, probability=TRUE)
table(predicted = svmpred, truth = data_test$Churn)
#..............................................................................
#             truth
# predicted   No   Yes
#        No  1383  260
#       Yes  163   297
#..............................................................................
confusionMatrix(svmpred, data_test$Churn, positive = "Yes")
#..............................................................................
# Confusion Matrix and Statistics
# -------------------------------
#             Reference
# predicted   No   Yes
#        No  1383  260
#       Yes  163   297
#
#            Accuracy : 0.7989
#              95% CI : (0.7772, 0.8121)
#
#          Sensitivity : 0.8946
#          Specificity : 0.5332
#
# 'Positive' Class : No
#..............................................................................
svmmodel.probs<-attr(svmpred,"names")
svmmodel.class<-predict(churn.svm.model, subset(data_test,select=-Churn), probability = TRUE)
svmmodel.labels<-data_test$Churn
#analyzing result
#roc analysis for test data
svmmodel.prediction<-prediction(svmmodel.probs,svmmodel.labels)
svmmodel.performance<-performance(svmmodel.prediction,"tpr","fpr")
svmmodel.auc<-performance(svmmodel.prediction,"auc")@y.values[[1]]
svmpred
str(svmpred)
svmmodel.probs<-attr(svmpred,"probabilities")
svmmodel.class<-predict(churn.svm.model, subset(data_test,select=-Churn), probability = TRUE)
svmmodel.labels<-data_test$Churn
#analyzing result
#roc analysis for test data
svmmodel.prediction<-prediction(svmmodel.probs,svmmodel.labels)
svmmodel.performance<-performance(svmmodel.prediction,"tpr","fpr")
svmmodel.auc<-performance(svmmodel.prediction,"auc")@y.values[[1]]
svmmodel.class<-predict(churn.svm.model, subset(data_test,select=-Churn), probability = TRUE)
svmmodel.labels<-data_test$Churn
svmmodel.prediction<-prediction(svmmodel.probs,svmmodel.labels)
svmmodel.performance<-performance(svmmodel.prediction,"tpr","fpr")
svmmodel.prediction<-prediction(svmmodel.probs,svmmodel.labels)
svmmodel.performance<-performance(svmmodel.prediction,"tpr","fpr")
svmmodel.auc<-performance(svmmodel.prediction,"auc")@y.values[[1]]
length(svmmodel.probs)
length(svmmodel.labels)
svmmodel.class<-predict(churn.svm.model, subset(data_test,select=-Churn), probability = TRUE)
svmmodel.labels<-data_test$Churn
svmmodel.prediction <- prediction(svmmodel.probs, svmmodel.labels)
svmmodel.probs<-attr(svmpred, "probabilities")
svmmodel.class<-predict(churn.svm.model, subset(data_test,select=-Churn), type="class")
svmmodel.labels<-data_test$Churn
svmmodel.prediction <- prediction(svmmodel.probs, svmmodel.labels)
svmpred <- predict(churn.svm.model, data_test, probability=TRUE)
table(predicted = svmpred, truth = data_test$Churn)
#..............................................................................
#             truth
# predicted   No   Yes
#        No  1383  260
#       Yes  163   297
#..............................................................................
confusionMatrix(svmpred, data_test$Churn, positive = "Yes")
svmmodel.probs<-attr(svmpred, "probabilities")
svmmodel.class<-predict(churn.svm.model, subset(data_test,select=-Churn), type="class")
svmmodel.labels<-data_test$Churn
svmmodel.prediction <- prediction(svmmodel.probs, svmmodel.labels)
svmmodel.performance <- performance(svmmodel.prediction,"tpr","fpr")
svmmodel.auc<-performance(svmmodel.prediction,"auc")@y.values[[1]]
length(svmmodel.labels)
length(svmmodel.probs)
summary(churn.svm.model)
str(churn.svm.model)
str(churn.svm.model$probA)
str(churn.svm.model$probB)
svmmodel.probs1<-attr(svmmodel.predict,"decision.values")
svmmodel.probs1<-attr(svmpred,"decision.values")
str(svmmodel.probs1)
